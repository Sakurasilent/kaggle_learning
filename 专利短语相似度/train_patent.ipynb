{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\Administrator\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\glue\\479ff3f18b7b4e5a529ea7117f3ce02d081a7219ee662384b5633d830a11b981 (last modified on Wed Aug 24 18:12:12 2022) since it couldn't be found locally at D:\\information\\code\\pycharmProjects\\kaggle_learning\\专利短语相似度\\glue\\glue.py, or remotely (ConnectionError).\n"
     ]
    },
    {
     "data": {
      "text/plain": "Metric(name: \"glue\", features: {'predictions': Value(dtype='float32', id=None), 'references': Value(dtype='float32', id=None)}, usage: \"\"\"\nCompute GLUE evaluation metric associated to each GLUE dataset.\nArgs:\n    predictions: list of predictions to score.\n        Each translation should be tokenized into a list of tokens.\n    references: list of lists of references for each translation.\n        Each reference should be tokenized into a list of tokens.\nReturns: depending on the GLUE subset, one or several of:\n    \"accuracy\": Accuracy\n    \"f1\": F1 score\n    \"pearson\": Pearson Correlation\n    \"spearmanr\": Spearman Correlation\n    \"matthews_correlation\": Matthew Correlation\nExamples:\n\n    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n    >>> references = [0, 1]\n    >>> predictions = [0, 1]\n    >>> results = glue_metric.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'accuracy': 1.0}\n\n    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n    >>> references = [0, 1]\n    >>> predictions = [0, 1]\n    >>> results = glue_metric.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'accuracy': 1.0, 'f1': 1.0}\n\n    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n    >>> references = [0., 1., 2., 3., 4., 5.]\n    >>> predictions = [0., 1., 2., 3., 4., 5.]\n    >>> results = glue_metric.compute(predictions=predictions, references=references)\n    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n    {'pearson': 1.0, 'spearmanr': 1.0}\n\n    >>> glue_metric = datasets.load_metric('glue', 'cola')\n    >>> references = [0, 1]\n    >>> predictions = [0, 1]\n    >>> results = glue_metric.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'matthews_correlation': 1.0}\n\"\"\", stored examples: 0)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('glue', 'stsb')\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = 'microsoft/deberta-v3-small'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': [1, 12018, 2985, 261, 291, 311, 4378, 300, 2, 263, 291, 269, 327, 266, 4378, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"hellow, this one sentence!\", \"and this is also a sentence.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_path = 'data/uspatent/'\n",
    "df = pd.read_csv(input_path+'train.csv')\n",
    "df_title = pd.read_csv(input_path+'titles.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                     id        anchor                  target context  score\n0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n...                 ...           ...                     ...     ...    ...\n36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n36471  756ec035e694722b  wood article         wooden material     B44   0.75\n36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n\n[36473 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36d72442aefd8232</td>\n      <td>abatement</td>\n      <td>active catalyst</td>\n      <td>A47</td>\n      <td>0.25</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36468</th>\n      <td>8e1386cbefd7f245</td>\n      <td>wood article</td>\n      <td>wooden article</td>\n      <td>B44</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <th>36469</th>\n      <td>42d9e032d1cd3242</td>\n      <td>wood article</td>\n      <td>wooden box</td>\n      <td>B44</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>36470</th>\n      <td>208654ccb9e14fa3</td>\n      <td>wood article</td>\n      <td>wooden handle</td>\n      <td>B44</td>\n      <td>0.50</td>\n    </tr>\n    <tr>\n      <th>36471</th>\n      <td>756ec035e694722b</td>\n      <td>wood article</td>\n      <td>wooden material</td>\n      <td>B44</td>\n      <td>0.75</td>\n    </tr>\n    <tr>\n      <th>36472</th>\n      <td>8d135da0b55b8c88</td>\n      <td>wood article</td>\n      <td>wooden substrate</td>\n      <td>B44</td>\n      <td>0.50</td>\n    </tr>\n  </tbody>\n</table>\n<p>36473 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                code                                              title  \\\n0                  A                                  HUMAN NECESSITIES   \n1                A01  AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...   \n2               A01B  SOIL WORKING IN AGRICULTURE OR FORESTRY; PARTS...   \n3           A01B1/00  Hand tools (edge trimmers for lawns A01G3/06  ...   \n4           A01B1/02  Spades; Shovels {(hand-operated dredgers E02F3...   \n...              ...                                                ...   \n260471  Y10T483/1864                      including tool pot or adapter   \n260472  Y10T483/1873                                    Indexing matrix   \n260473  Y10T483/1882                                        Rotary disc   \n260474  Y10T483/1891                                      Chain or belt   \n260475    Y10T483/19                                      Miscellaneous   \n\n       section  class subclass  group  main_group  \n0            A    NaN      NaN    NaN         NaN  \n1            A    1.0      NaN    NaN         NaN  \n2            A    1.0        B    NaN         NaN  \n3            A    1.0        B    1.0         0.0  \n4            A    1.0        B    1.0         2.0  \n...        ...    ...      ...    ...         ...  \n260471       Y   10.0        T  483.0      1864.0  \n260472       Y   10.0        T  483.0      1873.0  \n260473       Y   10.0        T  483.0      1882.0  \n260474       Y   10.0        T  483.0      1891.0  \n260475       Y   10.0        T  483.0        19.0  \n\n[260476 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>code</th>\n      <th>title</th>\n      <th>section</th>\n      <th>class</th>\n      <th>subclass</th>\n      <th>group</th>\n      <th>main_group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A</td>\n      <td>HUMAN NECESSITIES</td>\n      <td>A</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A01</td>\n      <td>AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTI...</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A01B</td>\n      <td>SOIL WORKING IN AGRICULTURE OR FORESTRY; PARTS...</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>B</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A01B1/00</td>\n      <td>Hand tools (edge trimmers for lawns A01G3/06  ...</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>B</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A01B1/02</td>\n      <td>Spades; Shovels {(hand-operated dredgers E02F3...</td>\n      <td>A</td>\n      <td>1.0</td>\n      <td>B</td>\n      <td>1.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>260471</th>\n      <td>Y10T483/1864</td>\n      <td>including tool pot or adapter</td>\n      <td>Y</td>\n      <td>10.0</td>\n      <td>T</td>\n      <td>483.0</td>\n      <td>1864.0</td>\n    </tr>\n    <tr>\n      <th>260472</th>\n      <td>Y10T483/1873</td>\n      <td>Indexing matrix</td>\n      <td>Y</td>\n      <td>10.0</td>\n      <td>T</td>\n      <td>483.0</td>\n      <td>1873.0</td>\n    </tr>\n    <tr>\n      <th>260473</th>\n      <td>Y10T483/1882</td>\n      <td>Rotary disc</td>\n      <td>Y</td>\n      <td>10.0</td>\n      <td>T</td>\n      <td>483.0</td>\n      <td>1882.0</td>\n    </tr>\n    <tr>\n      <th>260474</th>\n      <td>Y10T483/1891</td>\n      <td>Chain or belt</td>\n      <td>Y</td>\n      <td>10.0</td>\n      <td>T</td>\n      <td>483.0</td>\n      <td>1891.0</td>\n    </tr>\n    <tr>\n      <th>260475</th>\n      <td>Y10T483/19</td>\n      <td>Miscellaneous</td>\n      <td>Y</td>\n      <td>10.0</td>\n      <td>T</td>\n      <td>483.0</td>\n      <td>19.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>260476 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_title"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'9022'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3079\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3080\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3081\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\index.pyx\u001B[0m in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001B[0m in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: '9022'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24656\\2530525455.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf_title\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'code'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'class'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'9022'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pandas\\core\\frame.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3022\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnlevels\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3023\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_getitem_multilevel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3024\u001B[1;33m             \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3025\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mis_integer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3026\u001B[0m                 \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mindexer\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001B[0m in \u001B[0;36mget_loc\u001B[1;34m(self, key, method, tolerance)\u001B[0m\n\u001B[0;32m   3080\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_loc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcasted_key\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3081\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mKeyError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3082\u001B[1;33m                 \u001B[1;32mraise\u001B[0m \u001B[0mKeyError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0merr\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3083\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3084\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtolerance\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: '9022'"
     ]
    }
   ],
   "source": [
    "df_title[['code', 'class']]['9022']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(Index(['id', 'anchor', 'target', 'context', 'score'], dtype='object'),\n Index(['code', 'title', 'section', 'class', 'subclass', 'group', 'main_group'], dtype='object'))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, df_title.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df = df.merge(df_title, how='left', left_on='context', right_on='code')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "(Index(['id', 'anchor', 'target', 'context', 'score', 'code', 'title',\n        'section', 'class', 'subclass', 'group', 'main_group'],\n       dtype='object'),\n Index(['code', 'title', 'section', 'class', 'subclass', 'group', 'main_group'], dtype='object'))"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns, df_title.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = df[['id', 'anchor', 'target', 'context', 'score', 'title']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                     id        anchor                  target context  score  \\\n0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50   \n1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75   \n2      36d72442aefd8232     abatement         active catalyst     A47   0.25   \n3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50   \n4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00   \n...                 ...           ...                     ...     ...    ...   \n36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00   \n36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50   \n36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50   \n36471  756ec035e694722b  wood article         wooden material     B44   0.75   \n36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50   \n\n                                                   title  \n0      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  \n1      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  \n2      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  \n3      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  \n4      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...  \n...                                                  ...  \n36468                                    DECORATIVE ARTS  \n36469                                    DECORATIVE ARTS  \n36470                                    DECORATIVE ARTS  \n36471                                    DECORATIVE ARTS  \n36472                                    DECORATIVE ARTS  \n\n[36473 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36d72442aefd8232</td>\n      <td>abatement</td>\n      <td>active catalyst</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36468</th>\n      <td>8e1386cbefd7f245</td>\n      <td>wood article</td>\n      <td>wooden article</td>\n      <td>B44</td>\n      <td>1.00</td>\n      <td>DECORATIVE ARTS</td>\n    </tr>\n    <tr>\n      <th>36469</th>\n      <td>42d9e032d1cd3242</td>\n      <td>wood article</td>\n      <td>wooden box</td>\n      <td>B44</td>\n      <td>0.50</td>\n      <td>DECORATIVE ARTS</td>\n    </tr>\n    <tr>\n      <th>36470</th>\n      <td>208654ccb9e14fa3</td>\n      <td>wood article</td>\n      <td>wooden handle</td>\n      <td>B44</td>\n      <td>0.50</td>\n      <td>DECORATIVE ARTS</td>\n    </tr>\n    <tr>\n      <th>36471</th>\n      <td>756ec035e694722b</td>\n      <td>wood article</td>\n      <td>wooden material</td>\n      <td>B44</td>\n      <td>0.75</td>\n      <td>DECORATIVE ARTS</td>\n    </tr>\n    <tr>\n      <th>36472</th>\n      <td>8d135da0b55b8c88</td>\n      <td>wood article</td>\n      <td>wooden substrate</td>\n      <td>B44</td>\n      <td>0.50</td>\n      <td>DECORATIVE ARTS</td>\n    </tr>\n  </tbody>\n</table>\n<p>36473 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "kf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "df['fold']=-1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "0           abatement\n1           abatement\n2           abatement\n3           abatement\n4           abatement\n             ...     \n36468    wood article\n36469    wood article\n36470    wood article\n36471    wood article\n36472    wood article\nName: anchor, Length: 36473, dtype: object"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['anchor']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 这里到吗不明白"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\sklearn\\model_selection\\_split.py:880: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "for  f, (t_, v_) in enumerate(kf.split(X=df, y=df['anchor'], groups=df['anchor'])):\n",
    "    # print('f', f)\n",
    "    # print('t', t_)\n",
    "    # print('v', v_)\n",
    "    df.loc[v_, 'fold'] = f"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                     id        anchor                  target context  score  \\\n0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50   \n1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75   \n2      36d72442aefd8232     abatement         active catalyst     A47   0.25   \n3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50   \n4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00   \n...                 ...           ...                     ...     ...    ...   \n36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00   \n36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50   \n36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50   \n36471  756ec035e694722b  wood article         wooden material     B44   0.75   \n36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50   \n\n                                                   title  fold  \n0      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     4  \n1      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     4  \n2      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     4  \n3      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     4  \n4      FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...     4  \n...                                                  ...   ...  \n36468                                    DECORATIVE ARTS     2  \n36469                                    DECORATIVE ARTS     2  \n36470                                    DECORATIVE ARTS     2  \n36471                                    DECORATIVE ARTS     2  \n36472                                    DECORATIVE ARTS     2  \n\n[36473 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>score</th>\n      <th>title</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37d61fd2272659b1</td>\n      <td>abatement</td>\n      <td>abatement of pollution</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7b9652b17b68b7a4</td>\n      <td>abatement</td>\n      <td>act of abating</td>\n      <td>A47</td>\n      <td>0.75</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36d72442aefd8232</td>\n      <td>abatement</td>\n      <td>active catalyst</td>\n      <td>A47</td>\n      <td>0.25</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5296b0c19e1ce60e</td>\n      <td>abatement</td>\n      <td>eliminating process</td>\n      <td>A47</td>\n      <td>0.50</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>54c1e3b9184cb5b6</td>\n      <td>abatement</td>\n      <td>forest region</td>\n      <td>A47</td>\n      <td>0.00</td>\n      <td>FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>36468</th>\n      <td>8e1386cbefd7f245</td>\n      <td>wood article</td>\n      <td>wooden article</td>\n      <td>B44</td>\n      <td>1.00</td>\n      <td>DECORATIVE ARTS</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>36469</th>\n      <td>42d9e032d1cd3242</td>\n      <td>wood article</td>\n      <td>wooden box</td>\n      <td>B44</td>\n      <td>0.50</td>\n      <td>DECORATIVE ARTS</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>36470</th>\n      <td>208654ccb9e14fa3</td>\n      <td>wood article</td>\n      <td>wooden handle</td>\n      <td>B44</td>\n      <td>0.50</td>\n      <td>DECORATIVE ARTS</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>36471</th>\n      <td>756ec035e694722b</td>\n      <td>wood article</td>\n      <td>wooden material</td>\n      <td>B44</td>\n      <td>0.75</td>\n      <td>DECORATIVE ARTS</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>36472</th>\n      <td>8d135da0b55b8c88</td>\n      <td>wood article</td>\n      <td>wooden substrate</td>\n      <td>B44</td>\n      <td>0.50</td>\n      <td>DECORATIVE ARTS</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>36473 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "df['input'] = df['anchor'] + tokenizer.sep_token + df['title'].apply(str.lower)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['input'].values.astype(str)\n",
    "        # 这个target要研究一下 题目要求 不知道作用\n",
    "        self.targets = df['target'].values.astype(str)\n",
    "        self.label = df['score'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = self.inputs[item]\n",
    "        targets = self.targets[item]\n",
    "        label = self.label[item]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            inputs,\n",
    "            targets,\n",
    "            padding = 'max_length',\n",
    "            truncation=True\n",
    "                           )\n",
    "        return {**inputs,\n",
    "                'label':torch.as_tensor(label, dtype=torch.float)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 模型微调\n",
    "### 这个numberlayer不明白 官方文档也没有"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.weight', 'classifier.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 1\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint,num_labels=num_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "metric_name = 'person'\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "batch_size = 200\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    save_total_limit=1,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def computer_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return metric.compute(predictions=predictions,references = labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(df[df['fold']!=0])\n",
    "val_dataset = TrainDataset(df[df['fold']==0])\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=computer_metrics,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 29429\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 200\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 200\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 740\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 300.00 MiB (GPU 0; 2.00 GiB total capacity; 1.05 GiB already allocated; 0 bytes free; 1.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_24656\\49973641.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1420\u001B[0m                         \u001B[0mtr_loss_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1421\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1422\u001B[1;33m                     \u001B[0mtr_loss_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1423\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1424\u001B[0m                 if (\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mtraining_step\u001B[1;34m(self, model, inputs)\u001B[0m\n\u001B[0;32m   2009\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2010\u001B[0m         \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautocast_smart_context_manager\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2011\u001B[1;33m             \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompute_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2012\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2013\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mn_gpu\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mcompute_loss\u001B[1;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[0;32m   2041\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2042\u001B[0m             \u001B[0mlabels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2043\u001B[1;33m         \u001B[0moutputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   2044\u001B[0m         \u001B[1;31m# Save past state if it exists\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   2045\u001B[0m         \u001B[1;31m# TODO: this needs to be fixed and made cleaner later.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1284\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1285\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_hidden_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1286\u001B[1;33m             \u001B[0mreturn_dict\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1287\u001B[0m         )\n\u001B[0;32m   1288\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1052\u001B[0m             \u001B[0moutput_hidden_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1053\u001B[0m             \u001B[0moutput_attentions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1054\u001B[1;33m             \u001B[0mreturn_dict\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mreturn_dict\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1055\u001B[0m         )\n\u001B[0;32m   1056\u001B[0m         \u001B[0mencoded_layers\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder_outputs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001B[0m\n\u001B[0;32m    502\u001B[0m                     \u001B[0mrelative_pos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrelative_pos\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    503\u001B[0m                     \u001B[0mrel_embeddings\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrel_embeddings\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 504\u001B[1;33m                     \u001B[0moutput_attentions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0moutput_attentions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    505\u001B[0m                 )\n\u001B[0;32m    506\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001B[0m\n\u001B[0;32m    346\u001B[0m             \u001B[0mquery_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mquery_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    347\u001B[0m             \u001B[0mrelative_pos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrelative_pos\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 348\u001B[1;33m             \u001B[0mrel_embeddings\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrel_embeddings\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    349\u001B[0m         )\n\u001B[0;32m    350\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0moutput_attentions\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001B[0m\n\u001B[0;32m    277\u001B[0m             \u001B[0mquery_states\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mquery_states\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    278\u001B[0m             \u001B[0mrelative_pos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrelative_pos\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 279\u001B[1;33m             \u001B[0mrel_embeddings\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mrel_embeddings\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    280\u001B[0m         )\n\u001B[0;32m    281\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0moutput_attentions\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1108\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1111\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1112\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001B[0m\n\u001B[0;32m    697\u001B[0m             \u001B[0mrel_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpos_dropout\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrel_embeddings\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    698\u001B[0m             rel_att = self.disentangled_attention_bias(\n\u001B[1;32m--> 699\u001B[1;33m                 \u001B[0mquery_layer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey_layer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrelative_pos\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrel_embeddings\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mscale_factor\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    700\u001B[0m             )\n\u001B[0;32m    701\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramData\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\u001B[0m in \u001B[0;36mdisentangled_attention_bias\u001B[1;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001B[0m\n\u001B[0;32m    748\u001B[0m             ).repeat(query_layer.size(0) // self.num_attention_heads, 1, 1)\n\u001B[0;32m    749\u001B[0m             pos_key_layer = self.transpose_for_scores(self.key_proj(rel_embeddings), self.num_attention_heads).repeat(\n\u001B[1;32m--> 750\u001B[1;33m                 \u001B[0mquery_layer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m//\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnum_attention_heads\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    751\u001B[0m             )\n\u001B[0;32m    752\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 300.00 MiB (GPU 0; 2.00 GiB total capacity; 1.05 GiB already allocated; 0 bytes free; 1.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}